{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to graphreduce","text":"<p><code>graphreduce</code> is an abstraction layer for performing batch feature engineering.  </p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Cutomizable: abstractions allow feature implementations to be customized.  While many will opt for automated feature engineering, deduplication, anomalies, etc. may need custom or third-party library support.</li> <li>Interoperable: supports <code>pandas</code>, <code>pyspark</code>, <code>dask</code>, and <code>polars</code> dataframe APIs and SQL dialects for Redshift, BigQuery, Databricks SQL, Snowflake, and more.</li> <li>Composable: by using graphs as the underpinning data structure with <code>networkx</code> we allow arbitrarily large feature engineering pipelines by doing depth first traversal based on cardinality.</li> <li>Scalable: support for different computational backends and checkpointing allow for massive feature engineering graphs to be constructed and executed with a compute push down paradigm</li> <li>Automated: by leveraging and extending ideas from research such as Deep Feature Synthesis we support fully automated feature engineering.</li> <li>Point in time correctness: <code>graphreduce</code> requires time-series data to have a date key, allowing for point in time correctness filtering to be applied across all data / nodes in the compute graph.</li> <li>Production-ready: since batch feature engineering pipelines require a computational graph defined, this makes transition to production deployments </li> <li>Cardinality awareness: in most tabular datasets cardinality needs to be handled carefully to avoid duplication and proper aggregation - <code>graphreduce</code> makes  this a breeze.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#with-pip","title":"with pip","text":"<pre><code>pip install graphreduce\n</code></pre>"},{"location":"#from-source","title":"from source","text":"<pre><code>git clone https://github.com/wesmadrigal/graphreduce\ncd graphreduce &amp;&amp; pip install -e .\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":"<pre><code>import datetime\nimport pandas as pd\nfrom graphreduce.node import GraphReduceNode, DynamicNode\nfrom graphreduce.enum import ComputeLayerEnum, PeriodUnit\nfrom graphreduce.graph_reduce import GraphReduce\n\n# source from a csv file with the relationships\n# using the file at: https://github.com/wesmadrigal/GraphReduce/blob/master/examples/cust_graph_labels.csv\nreldf = pd.read_csv('cust_graph_labels.csv')\n\n# using the data from: https://github.com/wesmadrigal/GraphReduce/tree/master/tests/data/cust_data\nfiles = {\n    'cust.csv' : {'prefix':'cu'},\n    'orders.csv':{'prefix':'ord'},\n    'order_products.csv': {'prefix':'op'},\n    'notifications.csv':{'prefix':'notif'},\n    'notification_interactions.csv':{'prefix':'ni'},\n    'notification_interaction_types.csv':{'prefix':'nit'}\n\n}\n# create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=files[f]['prefix'],\n        date_key=None,\n        compute_layer=GraphReduceComputeLayerEnum.pandas,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in files.keys()\n}\ngr = GraphReduce(\n    name='cust_dynamic_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    auto_features=True,\n    auto_feature_hops_front=1,\n    auto_feature_hops_back=2,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day\n)\n# Add graph edges\nfor ix, row in reldf.iterrows():\n    gr.add_entity_edge(\n        parent_node=gr_nodes[row['to_name']],\n        relation_node=gr_nodes[row['from_name']],\n        parent_key=row['to_key'],\n        relation_key=row['from_key'],\n        reduce=True\n    )\n\n\ngr.do_transformations()\n2024-04-23 13:49:41 [info     ] hydrating graph attributes\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating graph data\n2024-04-23 13:49:41 [info     ] checking for prefix uniqueness\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notification_interaction_types.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] depth-first traversal through the graph from source: &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] Had label node &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] computed labels for &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n\ngr.parent_node.df\ncu_id   cu_name notif_customer_id   notif_id_count  notif_customer_id_count notif_ts_first  notif_ts_min    notif_ts_max    ni_notification_id_min  ni_notification_id_max  ni_notification_id_sum  ni_id_count_min ni_id_count_max ni_id_count_sum ni_notification_id_count_min    ni_notification_id_count_max    ni_notification_id_count_sum    ni_interaction_type_id_count_min    ni_interaction_type_id_count_max    ni_interaction_type_id_count_sum    ni_ts_first_first   ni_ts_first_min ni_ts_first_max ni_ts_min_first ni_ts_min_min   ni_ts_min_max   ni_ts_max_first ni_ts_max_min   ni_ts_max_max   ord_customer_id ord_id_count    ord_customer_id_count   ord_ts_first    ord_ts_min  ord_ts_max  op_order_id_min op_order_id_max op_order_id_sum op_id_count_min op_id_count_max op_id_count_sum op_order_id_count_min   op_order_id_count_max   op_order_id_count_sum   op_product_id_count_min op_product_id_count_max op_product_id_count_sum ord_customer_id_dupe    ord_id_label\n0   1   wes 1   6   6   2022-08-05  2022-08-05  2023-06-23  101.0   106.0   621.0   1.0 3.0 14.0    1.0 3.0 14.0    1.0 3.0 14.0    2022-08-06  2022-08-06  2023-05-15  2022-08-06  2022-08-06  2023-05-15  2022-08-08  2022-08-08  2023-05-15  1.0 2.0 2.0 2023-05-12  2023-05-12  2023-06-01  1.0 2.0 3.0 4.0 4.0 8.0 4.0 4.0 8.0 4.0 4.0 8.0 1.0 1.0\n1   2   john    2   7   7   2022-09-05  2022-09-05  2023-05-22  107.0   110.0   434.0   1.0 1.0 4.0 1.0 1.0 4.0 1.0 1.0 4.0 2023-06-01  2023-06-01  2023-06-04  2023-06-01  2023-06-01  2023-06-04  2023-06-01  2023-06-01  2023-06-04  2.0 1.0 1.0 2023-01-01  2023-01-01  2023-01-01  3.0 3.0 3.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 NaN NaN\n2   3   ryan    3   2   2   2023-06-12  2023-06-12  2023-09-01  NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaT NaT NaT NaT NaT NaT NaT NaT NaT 3.0 1.0 1.0 2023-06-01  2023-06-01  2023-06-01  5.0 5.0 5.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 NaN NaN\n3   4   tianji  4   2   2   2024-02-01  2024-02-01  2024-02-15  NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0\n</code></pre>"},{"location":"abstractions/","title":"Abstractions","text":""},{"location":"abstractions/#node-abstraction","title":"Node abstraction","text":"<p>We represent files and tables as nodes.  A node could be a csv file on your laptop, a parquet file in s3, or a Snowflake table in the cloud.</p> <p>We parameterize the data location, a string prefix so we know where the data originates, a primary key, a date key (if any), a compute layer (e.g., <code>pandas</code>, <code>dask</code>), and some other optional parameters to intantiate these.</p>"},{"location":"abstractions/#graphreducenode","title":"GraphReduceNode","text":"<p>The base <code>GraphReduceNode</code> requires the following abstract methods be defined * <code>do_filters</code> - all filter operations for this node go here (e.g., <code>df.filter...</code>)</p> <ul> <li> <p><code>do_annotate</code> - all annotations go here (e.g., <code>df['zip'] = df.zipfull.apply(lambda x: x.split('-')[0])</code>)</p> </li> <li> <p><code>do_post_join_annotate</code> - annotations that require data from a child be joined in (e.g., need a delta between dates from two tables)</p> </li> <li> <p><code>do_normalize</code> - all anomaly filtering, data normalization, etc. go here (e.g., <code>df['val_norm'] = df['val'].apply(lambda x: x/df['val'].max())</code>)</p> </li> <li> <p><code>do_post_join_filters</code> - all filters requiring data from more than 1 table go here</p> </li> <li> <p><code>do_reduce</code> - all aggregation operations for features (e.g., <code>df.groupby(key).agg(...)</code>)</p> </li> <li> <p><code>do_labels</code> - any label-specific aggregation operations (e.g., <code>df.groupby(key).agg(had_order = 1)</code>)</p> </li> </ul>"},{"location":"abstractions/#dynamicnode","title":"DynamicNode","text":"<p>A dynamic node is any node that is instantiated without defined method  definitions.  This is useful for doing automated feature engineering.</p>"},{"location":"abstractions/#sqlnode","title":"SQLNode","text":"<p>A SQL node is an abstraction for SQL dialects and backends.  This allows us to go beyond the dataframe API that a typical <code>GraphReduceNode</code> or <code>DynamicNode</code>  is built for and leverage a number of SQL backends.  There is more detail about how to use these in the SQL backends tutorial.</p>"},{"location":"abstractions/#edge","title":"Edge","text":"<p>An edge is a relationship between two nodes.  This is typically a foreign key.  For  example if we had a <code>customers</code> table and <code>orders</code> table we would add an edge between the  <code>customers</code> node and the <code>orders</code> node:</p> <pre><code>gr.add_entity_edge(\n    parent_node=customer_node,\n    relation_node=orders_node,\n    parent_key='id',\n    relation_key='customer_id',\n    reduce=True\n</code></pre> <p>The <code>reduce</code> parameter tells <code>graphreduce</code> whether or not to execute  aggregation operations.</p>"},{"location":"abstractions/#graphreduce-graph","title":"GraphReduce Graph","text":"<p>The top-level <code>GraphReduce</code> class inherits directly from <code>networkx.DiGraph</code> to take advantage of many graph algorithms implemented in <code>networkx</code>.  The instances house to shared parameters for the entire graph of computation across all nodes and edges.</p> <p>Things such as the node to which to aggregate the data, the date for splitting the data, the compute layer (e.g., <code>pandas</code>, <code>dask</code>), the amount of history to include (365 days), the label period, whether or not to automate feature engineering, the label/target node and label/target column, etc.  All of these parameters get pushed down through the graph so we can do things like point in time correctness, etc.</p> <p>Since we inherit from <code>networkx</code> the API for adding nodes is unchanged: <pre><code>import datetime\nfrom graphreduce.graph_reduce import GraphReduce\nfrom grpahreduce.enum import PeriodUnit, ComputeLayerEnum\n\ngr = GraphReduce(\n    name='test',\n    parent_node=customer_node,\n    fmt='parquet',\n    compute_layer=ComputeLayerEnum.spark,\n    cut_date=datetime.datetime(2024, 7, 1),\n    compute_period_val=365,\n    compute_period_unit=PeriodUnit.day\n    auto_features=False,\n    label_node=order_node,\n    label_operation='sum',\n    label_field='order_total',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day,\n    spark_sqlctx=sqlCtx\n)\n\ngr.add_node(customer_node)\ngr.add_node(order_node)\ngr.add_node(notification_node)\ngr.add_node(...)\n...\n</code></pre></p>"},{"location":"tutorial_custom_graph/","title":"Defining node-level operations","text":"<p>Full code here</p> <p>Many times the automated primitives aren't enough and we want custom aggregation, filtering, normalization, and annotation.  In these cases we need to define operations somewhere.  Graphreduce takes the approach of centralizing operations in the node to which they pertain.</p> <p>For example, if we are defining a filter operation on the <code>orders.csv</code> feature definition that will live in a node defined for that dataset:</p> <pre><code>from graphreduce.node import GraphReduceNode\n\nclass OrderNode(GraphReduceNode):\n    def do_filters(self):\n        self.df = self.df[self.df[self.colabbr('amount')] &lt; 100000]    \n</code></pre> <p>By defining a node per dataset we can implement custom logic and focus only on the data of interest versus line 355 of a 2000 line SQL statement.</p>"},{"location":"tutorial_custom_graph/#full-node-implementation","title":"Full node implementation","text":"<p><code>graphreduce</code> prioritizes convention over configuration, so all <code>GraphReduceNode</code> subclasses must define the 7 required abstract methods, even if they do nothing.  One of the main reasons for enforcing this is so that as feature definitions evolve the location in which a particular operation needs to go should be clear.</p> <pre><code>class OrderNode(GraphReduceNode):\n    def do_filters(self):\n        self.df = self.df[self.df[self.colabbr('amount')] &lt; 100000]\n\n    def do_annotate(self):\n        pass\n\n    def do_normalize(self):\n        pass\n\n    def do_reduce(self, reduce_key):\n        return self.prep_for_features().groupby(self.colabbr(reduce_key)).agg(\n            **{\n                self.colabbr('max_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='max'),\n                self.colabbr('min_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='min')\n            }\n        )\n\n    def do_labels(self, reduce_key):\n        pass\n\n    def do_post_join_annotate(self):\n        pass\n\n    def do_post_join_filters(self):\n        pass\n</code></pre>"},{"location":"tutorial_custom_graph/#reduce-operations","title":"Reduce operations","text":"<p>If we want any aggregation to happen on this node we need to define <code>do_reduce</code>.  In this case we are computing the <code>min</code> and <code>max</code> of the column called <code>amount</code>. There are two helper methods used in the above code snippet that deserve elaboration:</p> <ul> <li><code>self.colabbr</code> which is <code>GraphReduceNode.colabbr</code> - this method just uses the prefix parameterized for this node so a column like <code>'amount'</code> will now be <code>'ord_amount'</code> if the prefix is <code>'ord'</code></li> <li><code>self.prep_for_features</code> which is <code>GraphReduceNode.prep_for_features</code> - this method filters the dataframe by the <code>cut_date</code> and <code>compute_period_val</code> if the data is time series.  If the data is not time series it just returns the full dataframe.</li> </ul> <pre><code>    # By letting the `reduce_key` be \n    # a parameter we can aggregate to\n    # any arbitrary parent dimension.\n    def do_reduce(self, reduce_key):\n        return self.prep_for_features().groupby(self.colabbr(reduce_key)).agg(\n            **{\n                self.colabbr('num_orders'): pd.NamedAgg(column=self.colabbr(self.pk), aggfunc='count'),\n\n                self.colabbr('max_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='max'),\n                self.colabbr('min_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='min')\n            }\n        )\n</code></pre>"},{"location":"tutorial_custom_graph/#feature-generation-output","title":"Feature generation output","text":"<p>We can test these operations individually on an instantiated node. Recall that the <code>do_data</code> method just loads the data.  When we call the <code>do_reduce</code> method it will filter the dates.  We can see that <code>ord_customer_id</code> 1 had 3 orders in the time period and <code>ord_customer_id</code> 2 had 1 order in the time period. <pre><code>order_node = OrderNode(\n    fpath='orders.csv',\n    prefix='ord',\n    date_key='ts',\n    fmt='csv',\n    pk='id',\n    compute_layer=ComputeLayerEnum.pandas,\n    compute_period_val=180,\n    compute_period_unit=PeriodUnit.day,\n    cut_date=datetime.datetime(2023,10,1),\n    label_period_val=30,\n    label_period_unit=PeriodUnit.day\n)\norder_node.do_data()\nprint(order.do_reduce('customer_id'))\n\n                ord_num_orders  ord_max_amount  ord_min_amount\nord_customer_id         \n              1              3           1200            10\n              3              1            220           220\n\n\n                 ord_max_amount ord_min_amount\nord_customer_id     \n              1           1200  10\n              3           220   220\n</code></pre></p>"},{"location":"tutorial_custom_graph/#label-target-generation","title":"Label / target generation","text":"<p>When not using automated feature engineering we need to specify the label generation logic.  This can simply be selecting the label/target column and returning it, or something more complicated like a boolean flag of whether an event happened or another aggregation function.  </p> <p>To continue with this example, we'll be generating a label on the <code>orders.csv</code> file for whether or not a customer had an order in the future 30 days relative to a <code>cut_date</code> of October 1, 2023.</p> <p>The definition of the <code>do_labels</code> function now becomes <pre><code>class OrderNode(GraphReduceNode):\n    ...\n    ...\n\n    def do_labels(self, reduce_key):\n        label_df = self.prep_for_labels().groupby(self.colabbr(reduce_key)).agg(\n            **{\n                # We can subsequently turn this into a boolean\n                self.colabbr('label_orders'): pd.NamedAgg(column=self.colabbr(self.pk), aggfunc='count')\n            }\n        )\n        label_df[self.colabbr('label_had_order')] = label_df[self.colabbr('label_orders')].apply(lambda x: 1 if x &gt; 0 else 0)\n        return label_df\n\n    ...\n    ...\n</code></pre></p> <p>Now we can test the <code>do_labels</code> method with an instance: <pre><code>print(order_node.do_labels('customer_id'))\n\n                ord_label_orders    ord_label_had_order\nord_customer_id     \n              2            1                   1\n</code></pre></p>"},{"location":"tutorial_index/","title":"Quickstart","text":"<p>Full code here</p> <p>The most simple graph is two nodes with no time component, no aggregations, and no labels.  We are using customer (<code>cust.csv</code>) and order data (<code>orders.csv</code>).  </p> <p>In plain terms what the below code does is as follows:</p> <ol> <li>Create prefixes for each node so we always know where the column originated from after joinining the two datasets.</li> <li>Instantiate two <code>DynamicNode</code> instances: one node for <code>cust.csv</code> and another for <code>orders.csv</code>.</li> <li>Instantiate the <code>GraphReduce</code> object to house the compute graph.  We are specifying that the <code>cust.csv</code> node is the <code>parent_node</code>, which means all data will be joined to and aggregated to the <code>cust.csv</code> node.  In cases where we reduce the data the resulting dataset should be at the ganularity of the <code>parent_node</code> dimension.</li> <li>Add the nodes.</li> <li>Add the edge between the nodes.</li> <li>Execute the computation with <code>GraphReduce.do_transformations()</code> the primary entrypoint to execution.</li> <li>Dump out the head of the computed dataframe. </li> </ol> <pre><code>from graphreduce.node import GraphReduceNode, DynamicNode\nfrom graphreduce.graph_reduce import GraphReduce\nfrom graphreduce.enum import ComputeLayerEnum as GraphReduceComputeLayerEnum, PeriodUnit\n\n# Need unique prefixes for all nodes\n# so when columns are merged we know\n# where they originate from.\nprefixes = {\n    'cust.csv' : {'prefix':'cu'},\n    'orders.csv':{'prefix':'ord'}\n}\n\n# create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=prefixes[f]['prefix'],\n        date_key=None,\n        compute_layer=GraphReduceComputeLayerEnum.pandas,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in files.keys()\n}\n\n# Instantiate GraphReduce with params.\n# We are using 'cust.csv' as parent node\n# so the granularity should be at the customer\n# dimension.\ngr = GraphReduce(\n    name='starter_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    auto_features=True,\n    auto_feature_hops_front=1,\n    auto_feature_hops_back=2,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day\n)\n\n# Add the nodes to the GraphReduce instance.\ngr.add_node(gr_nodes['cust.csv'])\ngr.add_node(gr_nodes['orders.csv'])\n\ngr.add_entity_edge(\n    parent_node=gr_nodes['cust.csv'],\n    relation_node=gr_nodes['orders.csv'],\n    parent_key='id',\n    relation_key='customer_id',\n    reduce=True\n)\n\ngr.do_transformations()\n\ngr.parent_node.df.head()\n    cu_id   cu_name ord_customer_id ord_id_count    ord_customer_id_count   ord_ts_min  ord_ts_max  ord_amount_count    ord_customer_id_dupe    ord_id_label\n0   1   wes 1   3   3   2023-05-12  2023-09-02  3   1   3\n1   2   ana 2   3   3   2022-08-05  2023-10-15  3   2   3\n2   3   caleb   3   1   1   2023-06-01  2023-06-01  1   3   1\n3   4   luly    4   2   2   2024-01-01  2024-02-01  2   4   2\n</code></pre>"},{"location":"tutorial_pandas_dask/","title":"Pandas and Dask","text":""},{"location":"tutorial_sql_dialects/","title":"SQL backends and dialects","text":"<p>To use SQL dialects we need to use <code>SQLNode</code> instances or a subclass of <code>SQLNode</code>.  The following are implemented and available for use out of the box:</p> <ul> <li>Databricks SQL</li> <li>SQLite</li> <li>Redshift</li> <li>AWs Athena</li> <li>Snowflake (in progress)</li> </ul>"},{"location":"tutorial_sql_dialects/#sql-operations","title":"SQL operations","text":"<p>We provide an abstraction for defining individual SQL operations so that they become stackable.  Graphreduce compiles all of the SQL operations at runtime and pushes down to the provider.  </p> <p>The parent class <code>SQLNode</code> defines a method <code>build_query</code>, which takes an arbitrary number of <code>sqlop</code> instances and builds the query.  This abstraction allows for individual select statements, where clauses, group bys, etc. to be defined individually and at runtime <code>build_query</code> compiles them into an executable statement.</p> <p>An example <code>sqlop</code> would be a simple select: <pre><code>from graphreduce.models import sqlop\nfrom graphreduce.enum import SQLOpType\n\nsel = sqlop(optype=SQLOpType, opval=\"*\")\n</code></pre></p> <p>This select just says to <code>select *</code>.  We can stack a number of these and have <code>build_query</code> compile them for us.  We will continue with the <code>cust.csv</code> data but now it is stored in <code>sqlite</code>.</p> <pre><code>cust = SQLNode(fpath='cust',\n                prefix='cust',\n                client=conn, \n                compute_layer=ComputeLayerEnum.sqlite, \n                columns=['id','name'])\n\nprint(cust.build_query(\n    ops=[\n        sqlop(optype=SQLOpType.select, opval='id'),\n        sqlop(optype=SQLOpType.select, opval='name')\n    ]\n))\n\nSELECT id,name\n    FROM cust\nWHERE true\n</code></pre> <p>Since we parameterized the <code>SQLNode</code> instance with the <code>cust</code> table, our instance already knows which table to select from during every <code>sqlop</code> instance.  The <code>sqlop</code> tries to be the smallest unit of operation in SQL, allowing for stacking as many of them as you want.</p> <p>You can also chain the graphreduce methods and dynamically build up SQL like this:</p> <pre><code># Define the order node\nclass OrderNode(SQLNode):\n    def do_filters(self) -&gt; typing.List[sqlop]:\n        return [\n            sqlop(optype=SQLOpType.where, opval=f\"{self.colabbr(self.date_key)} &gt; '2022-12-01'\")\n        ]\n\n    def do_annotate(self) -&gt; typing.List[sqlop]:\n        pass\n\n    def do_normalize(self):\n        pass\n\n    def do_reduce(self, reduce_key):\n        return [\n            # Shouldn't this just be a select?\n            sqlop(optype=SQLOpType.aggfunc, opval=f\"count(*) as {self.colabbr('num_orders')}\"),\n            sqlop(optype=SQLOpType.agg, opval=f\"{self.colabbr(reduce_key)}\")\n        ]\n\n\n    def do_labels(self, reduce_key):\n        return [\n            sqlop(optype=SQLOpType.aggfunc, opval=f\"count(*) as {self.colabbr('num_orders_label')}\"),\n            sqlop(optype=SQLOpType.agg, opval=f\"{self.colabbr(reduce_key)}\")\n        ]\n\n# Instantiate\norder = OrderNode(\n    fpath='orders',\n    prefix='ord',\n    client=conn,\n    compute_layer=ComputeLayerEnum.sqlite,\n    columns=['id','customer_id','ts','amount'],\n    date_key='ts'\n)\n\n# build a query\nprint(order.build_query(\n    ops=order.do_filters() + order.do_reduce('customer_id')\n))\n\nSELECT ord_customer_id,\n        count(*) as ord_num_orders\n        FROM orders\n        WHERE ord_ts &gt; '2022-12-01'\n        GROUP BY ord_customer_id\n</code></pre>"},{"location":"tutorial_sql_dialects/#more-examples","title":"More examples","text":"<p>There are more examples on github</p> <ul> <li>example 1</li> <li>example 2</li> <li>example 3</li> <li>example 4 automated feature engineering</li> <li>example 5 automated feature engineering con't</li> </ul>"},{"location":"tutorial_swapping_compute/","title":"Swapping compute layers","text":"<p>Full code here</p> <p>There are API differences between a lot of compute layers but between <code>pandas</code> and <code>dask</code> the API is mostly the same.  This makes swapping between these two compute layers a breeze.</p> <p>Let's say we're using the <code>orders.csv</code> and <code>cust.csv</code> from prior examples and using automated feature engineering with <code>DynamicNode</code> instances.</p>"},{"location":"tutorial_swapping_compute/#pandas","title":"Pandas","text":"<p>We'll use the same <code>orders.csv</code> and <code>cust.csv</code> datasets and start with a <code>pandas</code> backend, which is specified with the <code>compute_layer</code> parameter.</p> <pre><code>prefixes = {\n    'cust.csv' : {'prefix':'cu'},\n    'orders.csv':{'prefix':'ord'}\n}\n\n# create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=prefixes[f]['prefix'],\n        date_key=None,\n        compute_layer=ComputeLayerEnum.pandas,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in prefixes.keys()\n}\n\ngr_nodes['cust.csv'].do_data()\ntype(gr_nodes['cust.csv'].df)\npandas.core.frame.DataFrame\n</code></pre>"},{"location":"tutorial_swapping_compute/#dask","title":"Dask","text":"<p>Now we can instantiate the same nodes with <code>dask</code>: <pre><code># create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=prefixes[f]['prefix'],\n        date_key=None,\n        compute_layer=ComputeLayerEnum.dask,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in prefixes.keys()\n}\n\ngr_nodes['cust.csv'].do_data()\ntype(gr_nodes['cust.csv'].df)\ndask.dataframe.core.DataFrame\n</code></pre></p>"},{"location":"tutorial_swapping_compute/#spark","title":"Spark","text":"<p>For <code>spark</code> you will need access to a SparkContext and can instantiate as follows: <pre><code>cloud_node = DynamicNode(\n    fpath='s3://mybucket/path/to/file.parquet',\n    fmt='parquet',\n    pk='id',\n    prefix='fi',\n    date_key='updated_at',\n    compute_layer=ComputeLayerEnum.spark,\n    compute_period_val=365,\n    compute_period_unit=PeriodUnit.day,\n    spark_sqlctx=sqlCtx\n)\ncloud_node.do_data()\ntype(cloud_node.df)\npyspark.sql.dataframe.DataFrame\n</code></pre></p> <p>To use SQL dialect we need to use the <code>SQLNode</code> class and it's subclasses.</p>"},{"location":"tutorial_time/","title":"Point in time correctness","text":"<p>Full code here</p> <p>To handle point in time correctness properly all nodes  in the graph need to share the same date parameters.  Also, all nodes must have a <code>date_key</code> set if they are to take advantage of the date filtering provided out of the box.</p>"},{"location":"tutorial_time/#a-single-node-date-filter","title":"A single node date filter","text":"<p>A good way to test if things are working properly is instantiate a single node and test the following functions:</p> <ul> <li><code>prep_for_features()</code> - this filters all data prior to a <code>cut_date</code> for generating features</li> <li><code>prep_for_labels()</code> - this filters all data after a <code>cut_date</code> for generating labels / targets</li> </ul> <pre><code>from graphreduce.node import DynamicNode\nfrom graphreduce.enum import ComputeLayerEnum, PeriodUnit\n# Only works in jupyter notebook \n!cat orders.csv\n\nid,customer_id,ts,amount\n1,1,2023-05-12,10\n2,1,2023-06-01,12\n3,2,2023-01-01,13\n4,2,2022-08-05,150\n5,3,2023-06-01,220\n6,1,2023-09-02,1200\n7,2,2023-10-15,47\n8,4,2024-01-01,42\n9,4,2024-02-01,42\n\norder_node = DynamicNode(\n    fpath='./orders.csv',\n    fmt='csv',\n    pk='id',\n    prefix='ord',\n    date_key='ts',\n    compute_layer=ComputeLayerEnum.pandas,\n    compute_period_val=180,\n    compute_period_unit=PeriodUnit.day,\n    cut_date=datetime.datetime(2023, 10, 1)\n)\n\n# loads the data\norder_node.do_data()\n\nprint(len(order_node.df))\n9\n\nprint(len(order_node.prep_for_features()))\n4\n\nprint(len(order_node.prep_for_labels()))\n1\n</code></pre> <p>In the above snippet we used a <code>cut_date</code> of October 1, 2023 and a <code>compute_period_val</code> of 180, so we want 180 day of history prior to October 1, 2023.  There are exactly 4 records that satisfy that criteria, so we can see the <code>prep_for_features</code> function is working as expected.</p>"},{"location":"tutorial_time/#putting-it-all-together","title":"Putting it all together","text":"<p>Additionally, for the labels we see there is 1 record within 30 days of October 1, 2023 so we can see the <code>prep_for_labels</code> function is working as expected. Using the example from before with <code>cust.csv</code> and <code>orders.csv</code> let's say we want to only compute features within 6 months and compute a label for 45 days.  </p> <p>In the <code>GraphReduce</code> instance we specify <code>compute_period_val</code> and <code>label_period_val</code>. These parameters control how much history is included during execution.  For this graph data from <code>2023/9/1</code> going back 180 days will be included in feature preparation and data from <code>2023/9/1</code> going forward 45 days will be included in label preparation.</p> <p><pre><code>gr = GraphReduce(\n    name='starter_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    compute_period_val=180,\n    compute_period_unit=PeriodUnit.day,\n    auto_features=True,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=45,\n    label_period_unit=PeriodUnit.day\n)\n</code></pre> This interface allows us to simply change a couple of parameters to regenerate datasets with different time periods.</p>"}]}