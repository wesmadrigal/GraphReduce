{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kurve","text":"<p>Self-serve demo available at https://demo.kurve.ai</p>"},{"location":"#deployment","title":"Deployment","text":"<p>Kurve is deployed into your environment behind your VPC so no data ever leaves your network.</p>"},{"location":"#metadata-storage","title":"Metadata storage","text":"<p>The Kurve application is configured with <code>sqlite</code> by default but needs a relational database for the application-level models.  The metadata Kurve extracts and manages can also be optionally pushed directly into catalogs such as Unity Catalog.</p>"},{"location":"#authentication","title":"Authentication","text":"<p>Kurve supports basic auth and OAuth 2.0.  During a deployment, since Kurve runs within your cloud, there may be a manual configuration of the internal callback URL.</p> <p>An example might be a VPC-protected internal subdomain: <code>CUSTOMER.kurve.dev/callback</code></p>"},{"location":"#metadata-extraction-and-inference","title":"Metadata extraction and inference","text":"<p>Kurve providers a number of algorithms that point at data catalogs and infer the relational structure. The metadata Kurve automatically extracts are:</p>"},{"location":"#per-table","title":"Per table","text":"<ol> <li>primary key</li> <li>date key</li> <li>row count</li> </ol>"},{"location":"#per-table-pair","title":"Per table pair","text":"<ol> <li>join keys (if any)</li> <li>cardinality</li> </ol>"},{"location":"#semantic-views-and-indexes","title":"Semantic views and indexes","text":""},{"location":"#semantic-view-integration-with-snowflake","title":"semantic view integration with Snowflake","text":"<ul> <li>automatic generation of Snowflake semantic views</li> <li>event interval for time-series data</li> </ul>"},{"location":"#index-integration-with-databricks","title":"index integration with Databricks","text":"<ul> <li>automatic creation of indexes</li> </ul>"},{"location":"#metadata-graphs","title":"Metadata graphs","text":"<p>Kurve metadata graphs leverage networkx under the hood to represent tables as nodes and relationships between tables as edges.  This allows us to benefit from the whole field of graph theory and take advantage of the open source ecosystem around it.</p>"},{"location":"#compute-graphs","title":"Compute graphs","text":"<p>Metadata graphs are combined with compute graphs to run multi-table data integration.  Since the metadata are repesented in a graph it allows us to perform depth first search/traversal based on cardinality to integrate and aggregate data in a bottom up way (start at the high row count fact tables and aggregate upward).</p> <p>Compute graphs are supported by the following open source projects:</p> <ul> <li>sqlglot</li> <li>graphreduce</li> <li>torch frame</li> </ul>"},{"location":"#quickstart-demo","title":"Quickstart demo","text":"<ol> <li>Create an account on demo.kurve.ai</li> <li> <p>Build a metadata graph on the sample data source <code>/usr/local/lake/cust_data</code>.</p> <ul> <li>this will infer the primary keys, date keys, and foreign keys between the tables and you should have the following metadata graph:</li> </ul> <p></p> </li> <li> <p>Inspect the metadata graph to confirm the relationships are correct.</p> <ul> <li>if there are incorrect relationships you can easily remove them </li> <li>if edges were missed they can be added</li> </ul> <p></p> </li> <li> <p>Build a compute graph with the <code>cust.csv</code> as the parent node using the following parameters:</p> <ol> <li>name: customer sample test</li> <li>parent node: <code>cust.csv</code></li> <li>depth limit: 2</li> <li>compute period in days: 365</li> <li>cut date: 5/1/2023</li> <li>label period in days: 60</li> <li>label node: <code>orders.csv</code></li> <li>label field: <code>id</code></li> <li>label operation: count</li> </ol> <p></p> </li> <li> <p>You should now see a compute graph with the <code>cust.csv</code> at the top and the <code>orders.csv</code> colored yellow since it is the target node.</p> </li> <li>In the compute graph viewier click on Actions and then Execute</li> <li>Navigate to the home screen and you should see a data source under My data sources</li> <li>Click on the data source and then click on the table name that was created which should    be the lower cased and underscore separated name you used for the compute graph.     </li> </ol>"},{"location":"#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>The parent node indicates which table everything will be integrated to, in this case the <code>cust.csv</code>.</p> </li> <li> <p>The depth limit indicates how many joins away from the parent node are permissible, in this case tables within 2 joins are included.</p> </li> <li> <p>The cut date is the date around which to filter - so nothing after the cut date will be included in the data aggregation / integration process.</p> </li> <li> <p>The compute period indicates how far backwards relative to the cut date to look in all of the tables.</p> </li> <li> <p>The label period in days is how many days after the cut date to look for computing the label.</p> </li> <li> <p>The label node is the table to run the label generation operation on.</p> </li> <li> <p>The label field is the field in the label node to run the operation on.</p> </li> <li> <p>The label operation is the actual operation to run, in this case count.</p> </li> <li> <p>The execution does a depth first traversal through the graph, starting at the bottom and working it's way up to the parent node.  Each step of the way it applies date filters based on the cut date and performs aggregations prior to joining.  We'll dig more into how this works and how to customze compute graphs in the tutorial.</p> </li> </ol>"},{"location":"concepts/","title":"Abstractions","text":""},{"location":"concepts/#data-sources","title":"Data sources","text":"<p>A source of data, such as a Databricks catalog, a Redshift database and schema, or a Snowflake database and schema.  Data sources can be one or multiple schemas / namespaces.  In the Kurve UI they are represented as:</p> <p><code>{provider}://{tenant}/{namespace}?schema={schema}&amp;format={storage_format}</code></p> <p>For an S3 bucket called <code>bazooka</code> and a directory called <code>arms-sales</code> with parquet format we would have the following URI:</p> <p><code>s3://bazooka/arms-sales?format=parquet</code></p> <p>For a Snowflake account with a database called <code>bazooka</code>, and a schema called <code>arms-sales</code> we would have the following URI:</p> <p><code>snowflake://SOME-SNOWFLAKE-ACCOUNT/bazooka?schema=arms-sales&amp;format=relational</code></p>"},{"location":"concepts/#schema-graphs","title":"Schema graphs","text":"<p>A metadata representation of a data source which represents the tables or files as nodes and relationships, foreign keys, as edges.</p>"},{"location":"concepts/#nodes","title":"Nodes","text":"<p>Each node represents a single table or file.</p> <p>An example might be a table called <code>bazooka_sales</code>.</p>"},{"location":"concepts/#edges","title":"Edges","text":"<p>Edges define the foreign key between two nodes.</p> <p>An example would be a table called <code>bazooka_sales</code> and another table called <code>arms_dealers</code>.  The two tables, represented as nodes, are connected by the columns: <code>bazooka_sales.arms_dealer_id</code> which points to <code>arms_dealers.id</code>.</p> <p>This relationship is captured in the edge between <code>bazooka_sales</code> and <code>arms_dealers</code>.</p>"},{"location":"concepts/#compute-graphs","title":"Compute graphs","text":"<p>A compute graph is a subgraph of a schema graph and defines computation over the tables and relationships involved.</p> <p>The top-level compute graph houses the following parameters which are shared across all nodes:   - parent node   - cut date the compute period   - compute period   - label period   - label node   - label field   - label operation</p>"},{"location":"concepts/#compute-graph-execution-order-of-operations","title":"Compute graph execution order of operations","text":"<ol> <li>load and prefix data</li> <li>annotations</li> <li>filters</li> <li>reduce / aggregate<ol> <li>auto-apply time filters based on cut date, date key, and compute period</li> <li>apply reduce/aggregation operations</li> </ol> </li> <li>compute labels (if any)<ol> <li>auto-apply time filters based on cut date, date key, and label period</li> </ol> </li> <li>post-join annotations<ol> <li>annotations to run which require a child/foreign relation to be merged prior to running</li> </ol> </li> <li>post-join filters<ol> <li>filters to run which require a child/foreign relation to be merged prior to running</li> </ol> </li> </ol>"},{"location":"concepts/#compute-graph-nodes","title":"Compute graph nodes","text":"<p>Compute graph nodes allow us to define the following:</p> <ul> <li>the prefix to use for columns</li> <li>the date key to use (if any)</li> <li>the columns to include / exclude</li> <li>annotations</li> <li>filters</li> <li>reduce / aggregation operations</li> <li>labels / target generation operations for ML</li> <li>post join annotations</li> <li>post join filters</li> </ul>"},{"location":"concepts/#operations","title":"operations","text":""},{"location":"concepts/#annotations","title":"annotations","text":"<p>Annotations are any manipulation of existing columns to create new ones such as applying a length function to a string column, extracting json to add new columns, or computing some function over a numerical column in a non-aggregated way.</p> <p>In plain english we're widening the table by adding new columns.  Let's say we have a table with 2 columns: <code>id INT</code> and <code>name INT</code>.  Kurve takes care of compiling the end to end SQL at runtime we can simply add the select and not think about the rest of the query: <pre><code>select *, length(name) as name_length\n</code></pre> At runtime this will compile into a full query like: <pre><code>create some_stage_name as\nselect *, length(name) as name_length\nfrom table\n</code></pre> We added the <code>name_length</code> column which is derived from the <code>name</code> column, so now we have 3 columns instead of 2.</p>"},{"location":"concepts/#filters","title":"filters","text":"<p>Filters are any filter applied to the node.  Since order of operation matters, they can also be combined with annotations.  They can be written in isolation without the rest of SQL grammar: <pre><code>where lower(name) not like '%test%'\n</code></pre> At runtime this will compile into a full query like: <pre><code>select *\nfrom some_stage_name\nwhere lower(name) like '%test%'\n</code></pre></p>"},{"location":"concepts/#reduce-aggregate","title":"reduce / aggregate","text":"<p>Aggregations are any <code>group by</code> applied to the table prior to joining it to it's parent.  Kurve uses completely automated reduce operations powered by graphreduce but also allows full customizability.</p> <p>As with filters and annotations, reduce operations can be written in isolation without the where clauses and annotations as follows: <pre><code>select customer_id,\ncount(*) as num_orders\nfrom 'orders.csv'\ngroup by customer_id\n</code></pre></p>"},{"location":"concepts/#labels-target-variables","title":"labels / target variables","text":"<p>Labels are the target variable for machine learning problems. Based on the parameterized cut date and the label period the label node will get filtered to compute the label.</p> <p>For a label period of 90 days and a cut date of Jan 1, 2025 we would get the following auto-generated SQL for the <code>orders.csv</code>: <pre><code>select customer_id,\ncount(*) as num_orders_label\nfrom 'orders.csv'\nwhere ts &gt; cut_date\nand ts &lt; cut_date + INTERVAL '90 day'\ngroup by customer_id\n</code></pre></p> <p>This tells us if a customer had an order in the next 90 days.</p>"},{"location":"concepts/#post-join-annotations-advanced","title":"post-join annotations (advanced)","text":"<p>Same as annotations above but applied after the specified relations are merged.  An example might be a <code>DATEDIFF</code> applied to two columns: one from the parent table and the other from the child table.</p> <p>Check out the advanced examples for more detail.</p>"},{"location":"concepts/#post-join-filters-advanced","title":"post-join filters (advanced)","text":"<p>Same as filters above but applied after the specified relations are merged.  An example might be filtering after a <code>DATEDIFF</code> has been applied to two columns from different table after joining them together.</p> <p>Check out the advanced examples for more detail.</p>"},{"location":"concepts/#compute-graph-edges","title":"Compute graph edges","text":"<p>Relationships between table.  The most noteworthy attribute about edges, beyond the relationship they encode, is whether or not to reduce the edge or not.</p> <p>For one to many relationships there are times where it is desirable to preserve all rows in the child relation and still join, knowing it will cause duplication of the parent relationship.  In these cases we can specify <code>reduce = False</code>.</p>"},{"location":"concepts/#decomposing-data-problems-spanning-multiple-tables-with-graphs","title":"Decomposing data problems spanning multiple tables with graphs","text":""},{"location":"concepts/#thinking-at-the-node-level","title":"Thinking at the node-level","text":"<p>Kurve is built on the realization that most data analytics and AI data discovery and data preparation involves multiple tables.  The more tables involved the higher cognitive load due to sheer table counts, but also due to what has traditionally been spaghetti code or pipeline jungles required to munge all of this data.</p> <p>By leveraging Kurve's metadata graphs and compute subraphs we can visually see large subgraphs of multi-table data integration one table / node at a time. This allows the developer to focus on individual components, nodes, of a larger compute operation without thinking about every aspect simultaneously.</p> <p>Kurve abstracts and automates away a lot of the menial boilerplate that also adds to complexity of pipelines and queries, such as prefixing columns in tables, filtering on dates, joining tables, and aggregating / reducing one to many relations prior to a join.  Kurve provides completely automated approaches to this but also allows full cutomization when desired.  Think of it like autonomous driving for data: use the autonamy where you want and take control where you feel you need to.</p>"},{"location":"concepts/#how-compute-graphs-get-executed","title":"How compute graphs get executed","text":""},{"location":"concepts/#compute-pushdown","title":"Compute pushdown","text":"<p>As of writing, Kurve supports connectors to the following.</p> <ul> <li>amazon s3</li> <li>snowflake</li> <li>databricks</li> <li>amazon Redshift</li> <li>unity catalog</li> <li>polaris catalog</li> <li>amazon RDS</li> </ul>"},{"location":"concepts/#depth-first-traversal","title":"Depth-first traversal","text":"<p>Kurve uses depth first traversal during compute graph execution, starting at the bottom and recursively working back up to the top.</p>"},{"location":"concepts/#dynamic-sql-generation-and-prefixing","title":"Dynamic SQL generation and prefixing","text":"<p>Kurve leverages sqlglot for SQL parsing and helping with SQL prefixing based on the specified prefix.</p> <p>All table in a compute graph must have a prefix and the prefixes must be unique.  For example a customers table is typically prefixed with something like <code>'cust'</code>.  This allows us to know which table(s) the data originated from after it has been integrated.</p>"},{"location":"concepts/#temporary-references-and-views","title":"Temporary references and views","text":"<p>Behind the scenes when a compute graph gets executed Kurve is creating and referencing incremental temporary references to incremental transformations of data throughout the execution.  For a 2 node compute graph with no customized operations the execution flow would be as follows:</p> <ol> <li>load and prefix data and create reference for table 1</li> <li>load and prefix data and create reference for table 2</li> <li>aggregate and create reference of aggregated data for table 2</li> <li>join aggregated data from table 2 to table 1 and create updated reference for table 1</li> </ol> <p>This execution flow would have at least 4 temporary incremental references created, which capture every table-level and join-level maninpuluation to the data.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#relational-schema-inference","title":"Relational schema inference","text":"<p>This tutorial covers the basic features of extracting relational metadata from a directory of files.  The same process would be followed for a data lake / data warehouse.</p> <ol> <li>Log in to kurve demo</li> <li>Look under Sample data sources for <code>/usr/local/lake/cust_data</code> and click Create Graph</li> <li>In a few seconds the page should refresh and you can view the graph by clicking View Graph</li> <li>View all of the foreign key relationships between the 6 tables:<ol> <li><code>cust.csv</code> connects with <code>orders.csv</code> on <code>id -&gt; customer_id</code></li> <li><code>cust.csv</code> connects with <code>notifications.csv</code> on <code>id -&gt; customer_id</code></li> <li><code>orders.csv</code> connects with <code>order_products.csv</code> on <code>id -&gt; order_id</code></li> <li><code>notifications.csv</code> connects with <code>notification_interactions.csv</code> on <code>id -&gt; notification_id</code></li> <li><code>notification_interactions.csv</code> connects with <code>notification_interaction_types.csv</code> on <code>interaction_type_id -&gt; id</code></li> </ol> </li> </ol> <p>In this example Kurve points at the 6 csv files, runs an algorithm, and extracts all of the relational metadata. Next we'll look at how to run computation on these relational metadata graphs.</p>"},{"location":"examples/#orienting-schema-graphs-around-particular-nodes","title":"Orienting schema graphs around particular nodes","text":"<p>Most analytics and AI problems are oriented around a particular dimension.  With a graph structure we can easily manipuate the schema graph around the dimension of interest.  For this example we'll orient the <code>/usr/local/lake/cust_data</code> sample schema around the <code>cust.csv</code>.  To do this we'll assign the <code>cust.csv</code> as the parent node:</p> <ol> <li>Under Actions in the schema graph viewer click Assign Parent Node and select <code>cust.csv</code> with <code>depth=1</code>.</li> <li>You should have a subgraph of 3 tables now: <code>cust.csv</code>, <code>orders.csv</code>, and <code>notifications.csv</code></li> <li>Now that we've oriented the dataset around the <code>cust.csv</code> dimension let's get to compute / data integration. </li> </ol>"},{"location":"examples/#compute-graph-basics","title":"Compute graph basics","text":"<ol> <li>Visit the schema graph for <code>/usr/local/lake/cust_data</code>.</li> <li>Click Actions, Assign Parent Node, select <code>cust.csv</code> with <code>depth = 1</code>.</li> <li>Click Actions, Compute Graph and plug in the following values:<ol> <li>Name: kurve cust demo</li> <li>Parent Node: cust.csv</li> <li>Depth limit: 1</li> <li>Compute period in days: 365</li> <li>Cut date: 05/01/2023</li> <li>Label period in days: 90</li> <li>Label node: notifications.csv</li> <li>Label field: id</li> <li>Label operation: count </li> </ol> </li> <li>Notice the color coating of the parent node, cust.csv, and the label node, notifications.csv.</li> <li>Let's make sure our parameters are correct: click Actions, Show compute graph details.  Here is what it should look like: </li> <li>If all of that looks good now let's execute the compute graph:<ol> <li>click Actions</li> <li>click Execute compute graph</li> <li>navigate home</li> <li>you should now see a data source under My data sources, click it and find the <code>kurve_cust_demo_train</code> table or whatever name you used and click on the table</li> <li>notice all of the columns but we should only have 4 rows, this is all 3 tables aggregated and integrated to the <code>cust.csv</code> dimension with point-in-time correctness based on the above parameters. </li> </ol> </li> </ol>"},{"location":"examples/#node-level-operations-customization-basic","title":"Node-level operations customization (basic)","text":"<p>In the compute graph created in the above example we leaned into Kurve's automation, but let's customize some things and see the effect.</p> <ol> <li> <p>Visit the compute graph under <code>/usr/local/lake/cust_data</code></p> </li> <li> <p>Click the parent node <code>cust.csv</code></p> </li> <li> <p>Click Edit Node and under Annotations / special selects enter the following:</p> <pre><code>select *, length(name) as name_length\n</code></pre> </li> <li> <p>Under Filters enter the following:</p> <p><pre><code>where name_length &lt; 4\n</code></pre>  5. Execute the compute graph 6. Navigate home and click on the compute graph output table under My data sources and you shuld now see only 3 rows. </p> </li> </ol> <p>In summary, we annotated the <code>cust.csv</code> node with a new column called <code>name_length</code> and then filtered the dataset to only contain rows where <code>name_length &lt; 4</code>, which filtered one row.  This highlights how we can customize compute graphs in a basic way.  The next example will do this in a more advanced way.</p>"},{"location":"examples/#node-level-operations-customization-advanced","title":"Node-level operations customization (advanced)","text":"<p>Continuing with the same schema and compute graph from the prior examples.</p>"},{"location":"examples/#post-join-annotation","title":"post-join annotation","text":"<p>We need to compute the difference between the <code>notifications.csv</code> and the <code>orders.csv</code> timestamps, which will require they both be joined to the <code>cust.csv</code> and then a <code>DATEDIFF</code> operation needs to be called between the dates.  The order of operations is as follows:</p> <ol> <li>Join <code>orders.csv</code> to <code>cust.csv</code> without aggregating</li> <li>Join <code>notifications.csv</code> to <code>cust.csv</code> without aggregating</li> <li>Compute date difference between <code>orders.csv</code> timestamp and <code>notifications.csv</code> timestamp</li> </ol> <p>To accomplish this in Kurve we just need to specify a post-join annotation definition on the <code>cust.csv</code> as follows: </p> <p>Notice the highlighted portion of the screenshot which shows the nodes that the <code>post-join annotate</code> operation depends on.  This tells Kurve that to apply the defined operation under post-join annotate we need bboth the <code>orders.csv</code> and <code>notifictions.csv</code> merged first.  The SQL we're running is: <pre><code>select *,\nDATEDIFF('DAY', ord_ts, not_ts) as order_notification_timediff\n</code></pre></p> <p>Additionally, we need to update both edges to not be aggregated prior to join: </p> <p>Finally, we can execute the compute graph.  Once it is executed notice the newly added column in the dataset.  Also notice that we don't have just 4 rows anymore since we did not aggregate the child relationships!</p> <p></p>"},{"location":"examples/#post-join-filters","title":"post-join filters","text":"<p>In post-join annotation we created a new column that depended on 2 foreign relationships.  We'll define a filter which depends on the same 2 relationships and applies to the column created by post-join annotate. </p> <p>Now re-execute the compute graph and view the output.  There shouldn't be any negatives for <code>order_notification_timediff</code>: </p>"},{"location":"examples/#snowflake-semantic-views","title":"Snowflake semantic views","text":"<p>We're using the <code>SNOWFLAKE_SAMPLE_DATA.TPCH_SF1</code> database for this example, which is in all Snowflake accounts.  This is also one of the data sources used in the Snowflake semantic views documentation.</p>"},{"location":"examples/#connect-snowflake-source","title":"Connect snowflake source","text":"<p>Navigate to the data sources section of Kurve and add the Snowflake data source as shown in the screenshot: </p>"},{"location":"examples/#infer-relationships-between-snowflake-tables","title":"Infer relationships between snowflake tables","text":"<p>After adding the Snowflake data source we can see it under My data sources in the homepage.  Now we'll go ahead and click Create graph to have Kurve infer all of the primary keys, foreign keys, and date keys between tables: </p>"},{"location":"examples/#inspect-kurve-metadata-graph-output","title":"Inspect Kurve metadata graph output","text":"<p>After Kurve finishes running we can open the metadata graph and view what it extracted:</p> <ul> <li>Notice the icons next to the columns of the <code>ORDERS</code> table in the image.</li> <li>It looks like Kurve selected <code>O_ORDERKEY</code> as the primary key and <code>O_ORDERDATE</code> as the date key.</li> <li>Notice the relationships / edges on the right hand side.</li> <li>These are what we'll use to dynamically generate the Snowflake semantic view </li> </ul>"},{"location":"examples/#generate-the-snowflake-semantic-view","title":"Generate the Snowflake Semantic View","text":"<p>Under Actions you will see Semantic generator if the data source is Snowflake: </p> <p>After executing a pop up will return with the semantic view Snowflake SQL definition pre-populated with:</p> <ul> <li>primary keys for all tables in the graph</li> <li>relationships for all tables in the graph</li> <li>metrics, dimensions, and facts are left for the user to define </li> </ul> <p>Finally, we can copy/paste this into Snowflake, add final modifications, and execute: </p>"},{"location":"snowflake/","title":"Kurve snowflake native app","text":""},{"location":"snowflake/#architecture","title":"Architecture","text":""},{"location":"snowflake/#configurating-the-application","title":"Configurating the application","text":"<p>After you have installed the Kurve application you will need to run the following Snowflake SQL with a role with sufficient permissions (e.g., <code>ACCOUNTADMIN</code>).  This ensures the <code>KURVE</code> database and a schema where Kurve can write to are available and accessible to the application. Post-installation instructions:</p> <pre><code>```sql\n-- Create a database for Kurve to write to or use an existing one\n--CREATE DATABASE IF NOT EXISTS MY_OUTPUT_DB;\n\n-- grants the user needs to run after installation\nGRANT USAGE ON DATABASE MY_OUTPUT_DB TO APPLICATION &lt;application_name&gt;;\n\n-- create a schema if needed or use an existing one\n-- CREATE SCHEMA IF NOT EXISTS MY_OUTPUT_DB.MY_OUTPUT_SCHEMA;\n\n-- grant usage to the application on your output schema\nGRANT USAGE ON SCHEMA MY_OUTPUT_DB.MY_OUTPUT_SCHEMA TO APPLICATION &lt;application_name&gt;;\n\n-- grant other permissions on output schema to application\nGRANT CREATE TEMPORARY TABLE ON SCHEMA MY_OUTPUT_DB.MY_OUTPUT_SCHEMA TO APPLICATION &lt;application_name&gt;;\n\nGRANT CREATE TABLE ON SCHEMA MY_OUTPUT_DB.MY_OUTPUT_SCHEMA TO APPLICATION &lt;application_name&gt;;\n\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA MY_OUTPUT_DB.MY_OUTPUT_SCHEMA TO APPLICATION &lt;application_name&gt;;\n\n-- grant future ownership of Kurve-created table to your rule\n-- may need to use ACCOUNTADMIN or admin role...\nGRANT SELECT ON FUTURE TABLES IN SCHEMA MY_OUTPUT_DB.MY_OUTPUT_SCHEMA TO ROLE &lt;my_role&gt;;\n\n-- create a warehouse for the application to use\n-- MUST BE NAME 'KURVE_WAREHOUSE'!\nCREATE WAREHOUSE IF NOT EXISTS KURVE_WAREHOUSE\nWAREHOUSE_SIZE = 'X-SMALL';\n\n-- grant usage on kurve warehouse to app\nGRANT USAGE ON WAREHOUSE KURVE_WAREHOUSE TO APPLICATION &lt;application_name&gt;;\n\n-- grant the application privileges on SNOWFLAKE_SAMPLE_DATA for sample tests\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE_SAMPLE_DATA TO APPLICATION &lt;application_name&gt;;\n</code></pre>"},{"location":"snowflake/#checking-application-status","title":"Checking application status","text":"<pre><code>USE SCHEMA KURVE_APP.KURVE_CORE;\n\nUSE ROLE MY_ROLE_WITH_ACCESS;\n\nCALL kurve_app.kurve_core.service_status();\n</code></pre>"},{"location":"snowflake/#getting-the-application-endpoint","title":"Getting the application endpoint","text":"<pre><code>USE SCHEMA KURVE_APP.KURVE_CORE;\n\nUSE ROLE MY_ROLE_WITH_ACCESS;\n\nCALL kurve_app.kurve_core.service_endpoint();\n</code></pre>"},{"location":"snowflake/#schema-inference-on-sample-data","title":"Schema inference on sample data","text":"<p>After you have found the application endpoint and logged in you should see some Sample data sources.  You should see a sample data source that shows <code>/runtime/data/relbench/rel-stack</code>.  This is the relbench stack exchange dataset.</p> <p>To infer the relationships between these tables click Create Graph and execute with the following parameters: </p> <p>You should now see the following metadata graph </p>"},{"location":"snowflake/#compute-graphs","title":"Compute graphs","text":"<p>With the schema graph created earlier we can create a compute graph.  We're using the relbench stack exchange dataset.  For this compute graph we'll build the graph for the user badge problem of predicting if a user will get a badge in the next 90 days.</p> <p>To do this we need to orient the problem around the user dimension and include all tables within 2 joins.  The cut off date for this problem is 1/1/2021 and we'll look at 2 years of hisory so we get the following compute graph parameters:</p> <p></p>"},{"location":"snowflake/#leveraging-compute-graphs-for-analytics-and-ai","title":"Leveraging compute graphs for analytics and AI","text":"<p>In a snowpark session within Snowflake you should be able to run the following code to train a model on the dataset created from the compute graph.  The below snippet of code assumes the sample data from above was used.</p> <pre><code>import snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import (\n    col, lit, coalesce, expr, stddev, sum as ssum, avg, count\n)\nfrom snowflake.snowpark.types import (\n    IntegerType, LongType, ShortType, ByteType,\n    DecimalType, FloatType, DoubleType\n)\nfrom snowflake.ml.modeling.preprocessing import StandardScaler, LabelEncoder\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\n\n\ndef main(session: snowpark.Session):\n    # YOU MAY NEED TO CHANGE THE TABLE NAME\n    table_name = \"MY_OUTPUT_DB.MY_OUTPUT_SCHEMA.STACKEX_USER_BADGE\"\n    raw_label = \"BADG_ID_LABEL\"\n    enc_label = f\"{raw_label}_ENC\"\n\n    # 1) numeric features (exclude label)\n    schema = session.table(table_name).schema\n    numeric_types = (IntegerType, LongType, ShortType, ByteType, DecimalType, FloatType, DoubleType)\n    features = [\n        f.name for f in schema\n        if isinstance(f.datatype, numeric_types) and f.name.upper() != raw_label.upper()\n    ]\n    if not features:\n        raise ValueError(\"No numeric features found in table.\")\n\n    # 2) cast features to FLOAT; keep label; drop null labels\n    df = session.table(table_name).select(\n        *[coalesce(col(c).cast(\"FLOAT\"), lit(0)).alias(c) for c in features],\n        col(raw_label).alias(raw_label)\n    ).filter(col(raw_label).is_not_null())\n\n    # 3) encode label -&gt; 0..K-1 and **REPLACE** the original column, ensure INT dtype\n    le = LabelEncoder(input_cols=[raw_label], output_cols=[enc_label])\n    le.fit(df)  # fit on full set of labels is okay for the label column\n    df = le.transform(df)\n    df = df.drop(raw_label).with_column_renamed(enc_label, raw_label)\n    df = df.with_column(raw_label, col(raw_label).cast(\"INT\"))\n\n    # 4) split FIRST\n    df = df.with_column(\"random_split\", expr(\"RANDOM()\"))\n    train_df = df.filter(expr(\"random_split &lt;= 0.8\")).drop(\"random_split\")\n    test_df  = df.filter(expr(\"random_split &gt; 0.8\")).drop(\"random_split\")\n\n    # 5) drop zero-variance cols **based on train**\n    stds_row = train_df.agg(*[stddev(col(c)).alias(c) for c in features]).collect()[0].as_dict()\n    features = [c for c in features if stds_row.get(c) not in (None, 0, 0.0)]\n    if not features:\n        raise ValueError(\"All numeric features are constant or null in the training split.\")\n\n    # 6) scale explicitly and detect actual output cols\n    scaled_cols = [f\"{c}_scaled\" for c in features]\n    scaler = StandardScaler(input_cols=features, output_cols=scaled_cols)\n    scaler.fit(train_df)\n    train_scaled_full = scaler.transform(train_df)\n    test_scaled_full  = scaler.transform(test_df)\n\n    existing_scaled = [c for c in scaled_cols if c in train_scaled_full.columns]\n    input_cols = existing_scaled if existing_scaled else [c for c in features if c in train_scaled_full.columns]\n    if not input_cols:\n        raise ValueError(\"No input columns available after scaling/transform.\")\n\n    # 7) select features + encoded INT label\n    train_scaled = train_scaled_full.select(*input_cols, raw_label)\n    test_scaled  = test_scaled_full.select(*input_cols, raw_label)\n\n    # Ensure labels are 0..K-1 integers and set num_class accordingly\n    num_class = train_scaled.select(col(raw_label)).distinct().count()\n\n    clf = XGBClassifier(\n        input_cols=input_cols,\n        label_cols=[raw_label],\n        output_cols=[\"PREDICTION\"],\n        max_depth=5,\n        n_estimators=100,\n        num_class=num_class,  # explicit for multi-class\n        # objective could be set, e.g., objective=\"multi:softprob\"\n    )\n    clf.fit(train_scaled)\n\n    # --- Evaluate on test split ---\n    preds = clf.predict(test_scaled).select(raw_label, \"PREDICTION\")\n\n    # 1) Overall accuracy\n    accuracy_df = preds.select(\n        avg((col(\"PREDICTION\") == col(raw_label)).cast(\"int\")).alias(\"accuracy\")\n    )\n    print(\"=== Overall Accuracy ===\")\n    accuracy_df.show()\n\n    # 2) Confusion matrix (actual vs predicted)\n    confusion_df = (\n        preds.group_by(raw_label, col(\"PREDICTION\"))\n             .agg(count(lit(1)).alias(\"count\"))\n             .sort(raw_label, col(\"PREDICTION\"))\n    )\n    print(\"=== Confusion Matrix (actual vs predicted) ===\")\n    confusion_df.show(200)  # increase limit if many classes\n\n    # 3) Per-class precision, recall, F1, support\n    classes_df = preds.select(col(raw_label).alias(\"CLASS\")).distinct()\n\n    # Cross-join to compute TP/FP/FN per class\n    joined = preds.cross_join(classes_df)\n    per_class_counts = (\n        joined.select(\n            col(\"CLASS\"),\n            ((col(\"PREDICTION\") == col(\"CLASS\")) &amp; (col(raw_label) == col(\"CLASS\"))).cast(\"int\").alias(\"tp1\"),\n            ((col(\"PREDICTION\") == col(\"CLASS\")) &amp; (col(raw_label) != col(\"CLASS\"))).cast(\"int\").alias(\"fp1\"),\n            ((col(\"PREDICTION\") != col(\"CLASS\")) &amp; (col(raw_label) == col(\"CLASS\"))).cast(\"int\").alias(\"fn1\"),\n            (col(raw_label) == col(\"CLASS\")).cast(\"int\").alias(\"support1\"),\n        )\n        .group_by(\"CLASS\")\n        .agg(\n            ssum(col(\"tp1\")).alias(\"tp\"),\n            ssum(col(\"fp1\")).alias(\"fp\"),\n            ssum(col(\"fn1\")).alias(\"fn\"),\n            ssum(col(\"support1\")).alias(\"support\"),\n        )\n    )\n\n    metrics = (\n        per_class_counts\n        .with_column(\"precision\", expr(\"tp / NULLIF(tp + fp, 0)\"))\n        .with_column(\"recall\",    expr(\"tp / NULLIF(tp + fn, 0)\"))\n        .with_column(\"f1\",        expr(\"2 * precision * recall / NULLIF(precision + recall, 0)\"))\n        .sort(col(\"CLASS\"))\n    )\n\n    print(\"=== Per-class metrics ===\")\n    metrics.show(200)\n\n    # If you want weighted averages across classes:\n    weighted = (\n        metrics.select(\n            (col(\"precision\") * col(\"support\")).alias(\"w_p\"),\n            (col(\"recall\")    * col(\"support\")).alias(\"w_r\"),\n            (col(\"f1\")        * col(\"support\")).alias(\"w_f\"),\n            col(\"support\")\n        )\n        .agg(\n            ssum(col(\"w_p\")).alias(\"sum_wp\"),\n            ssum(col(\"w_r\")).alias(\"sum_wr\"),\n            ssum(col(\"w_f\")).alias(\"sum_wf\"),\n            ssum(col(\"support\")).alias(\"sum_support\")\n        )\n        .select(\n            (col(\"sum_wp\")/col(\"sum_support\")).alias(\"precision_weighted\"),\n            (col(\"sum_wr\")/col(\"sum_support\")).alias(\"recall_weighted\"),\n            (col(\"sum_wf\")/col(\"sum_support\")).alias(\"f1_weighted\")\n        )\n    )\n    print(\"=== Weighted (by support) precision/recall/F1 ===\")\n    weighted.show()\n\n    # Optional: if you also want probabilities/LogLoss/ROC-AUC (macro),\n    # call predict_proba and compute metrics similarly using the probability columns.\n    return weighted\n</code></pre>"}]}