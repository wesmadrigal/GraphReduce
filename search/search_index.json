{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to graphreduce","text":"<p><code>graphreduce</code> is an abstraction layer for performing batch feature engineering.  </p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Cutomizable: abstractions allow feature implementations to be customized.  While many will opt for automated feature engineering, deduplication, anomalies, etc. may need custom or third-party library support.</li> <li>Interoperable: supports <code>pandas</code>, <code>pyspark</code>, <code>dask</code>, and <code>polars</code> dataframe APIs and SQL dialects for Redshift, BigQuery, Databricks SQL, Snowflake, and more.</li> <li>Composable: by using graphs as the underpinning data structure with <code>networkx</code> we allow arbitrarily large feature engineering pipelines by doing depth first traversal based on cardinality.</li> <li>Scalable: support for different computational backends and checkpointing allow for massive feature engineering graphs to be constructed and executed with a compute push down paradigm</li> <li>Automated: by leveraging and extending ideas from research such as Deep Feature Synthesis we support fully automated feature engineering.</li> <li>Point in time correctness: <code>graphreduce</code> requires time-series data to have a date key, allowing for point in time correctness filtering to be applied across all data / nodes in the compute graph.</li> <li>Production-ready: since batch feature engineering pipelines require a computational graph defined, this makes transition to production deployments </li> <li>Cardinality awareness: in most tabular datasets cardinality needs to be handled carefully to avoid duplication and proper aggregation - <code>graphreduce</code> makes  this a breeze.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#with-pip","title":"with pip","text":"<pre><code>pip install graphreduce\n</code></pre>"},{"location":"#from-source","title":"from source","text":"<pre><code>git clone https://github.com/wesmadrigal/graphreduce\ncd graphreduce &amp;&amp; pip install -e .\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":"<pre><code>import datetime\nimport pandas as pd\nfrom graphreduce.node import GraphReduceNode, DynamicNode\nfrom graphreduce.enum import ComputeLayerEnum, PeriodUnit\nfrom graphreduce.graph_reduce import GraphReduce\n\n# source from a csv file with the relationships\n# using the file at: https://github.com/wesmadrigal/GraphReduce/blob/master/examples/cust_graph_labels.csv\nreldf = pd.read_csv('cust_graph_labels.csv')\n\n# using the data from: https://github.com/wesmadrigal/GraphReduce/tree/master/tests/data/cust_data\nfiles = {\n    'cust.csv' : {'prefix':'cu'},\n    'orders.csv':{'prefix':'ord'},\n    'order_products.csv': {'prefix':'op'},\n    'notifications.csv':{'prefix':'notif'},\n    'notification_interactions.csv':{'prefix':'ni'},\n    'notification_interaction_types.csv':{'prefix':'nit'}\n\n}\n# create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=files[f]['prefix'],\n        date_key=None,\n        compute_layer=GraphReduceComputeLayerEnum.pandas,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in files.keys()\n}\ngr = GraphReduce(\n    name='cust_dynamic_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    auto_features=True,\n    auto_feature_hops_front=1,\n    auto_feature_hops_back=2,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day\n)\n# Add graph edges\nfor ix, row in reldf.iterrows():\n    gr.add_entity_edge(\n        parent_node=gr_nodes[row['to_name']],\n        relation_node=gr_nodes[row['from_name']],\n        parent_key=row['to_key'],\n        relation_key=row['from_key'],\n        reduce=True\n    )\n\n\ngr.do_transformations()\n2024-04-23 13:49:41 [info     ] hydrating graph attributes\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating attributes for DynamicNode\n2024-04-23 13:49:41 [info     ] hydrating graph data\n2024-04-23 13:49:41 [info     ] checking for prefix uniqueness\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notification_interaction_types.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] running filters, normalize, and annotations for &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] depth-first traversal through the graph from source: &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=notification_interactions.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=notifications.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=order_products.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] reducing relation &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] performing auto_features on node &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] joining &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt; to &lt;GraphReduceNode: fpath=cust.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] Had label node &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n2024-04-23 13:49:41 [info     ] computed labels for &lt;GraphReduceNode: fpath=orders.csv fmt=csv&gt;\n\ngr.parent_node.df\ncu_id   cu_name notif_customer_id   notif_id_count  notif_customer_id_count notif_ts_first  notif_ts_min    notif_ts_max    ni_notification_id_min  ni_notification_id_max  ni_notification_id_sum  ni_id_count_min ni_id_count_max ni_id_count_sum ni_notification_id_count_min    ni_notification_id_count_max    ni_notification_id_count_sum    ni_interaction_type_id_count_min    ni_interaction_type_id_count_max    ni_interaction_type_id_count_sum    ni_ts_first_first   ni_ts_first_min ni_ts_first_max ni_ts_min_first ni_ts_min_min   ni_ts_min_max   ni_ts_max_first ni_ts_max_min   ni_ts_max_max   ord_customer_id ord_id_count    ord_customer_id_count   ord_ts_first    ord_ts_min  ord_ts_max  op_order_id_min op_order_id_max op_order_id_sum op_id_count_min op_id_count_max op_id_count_sum op_order_id_count_min   op_order_id_count_max   op_order_id_count_sum   op_product_id_count_min op_product_id_count_max op_product_id_count_sum ord_customer_id_dupe    ord_id_label\n0   1   wes 1   6   6   2022-08-05  2022-08-05  2023-06-23  101.0   106.0   621.0   1.0 3.0 14.0    1.0 3.0 14.0    1.0 3.0 14.0    2022-08-06  2022-08-06  2023-05-15  2022-08-06  2022-08-06  2023-05-15  2022-08-08  2022-08-08  2023-05-15  1.0 2.0 2.0 2023-05-12  2023-05-12  2023-06-01  1.0 2.0 3.0 4.0 4.0 8.0 4.0 4.0 8.0 4.0 4.0 8.0 1.0 1.0\n1   2   john    2   7   7   2022-09-05  2022-09-05  2023-05-22  107.0   110.0   434.0   1.0 1.0 4.0 1.0 1.0 4.0 1.0 1.0 4.0 2023-06-01  2023-06-01  2023-06-04  2023-06-01  2023-06-01  2023-06-04  2023-06-01  2023-06-01  2023-06-04  2.0 1.0 1.0 2023-01-01  2023-01-01  2023-01-01  3.0 3.0 3.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 NaN NaN\n2   3   ryan    3   2   2   2023-06-12  2023-06-12  2023-09-01  NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaT NaT NaT NaT NaT NaT NaT NaT NaT 3.0 1.0 1.0 2023-06-01  2023-06-01  2023-06-01  5.0 5.0 5.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 NaN NaN\n3   4   tianji  4   2   2   2024-02-01  2024-02-01  2024-02-15  NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0 NaN NaN 0.0\n</code></pre>"},{"location":"abstractions/","title":"Abstractions","text":""},{"location":"abstractions/#node-abstraction","title":"Node abstraction","text":"<p>We represent files and tables as nodes.  A node could be a csv file on your laptop, a parquet file in s3, or a Snowflake table in the cloud.</p> <p>We parameterize the data location, a string prefix so we know where the data originates, a primary key, a date key (if any), a compute layer (e.g., <code>pandas</code>, <code>dask</code>), and some other optional parameters to intantiate these.</p>"},{"location":"abstractions/#graphreducenode","title":"GraphReduceNode","text":"<p>The base <code>GraphReduceNode</code> requires the following abstract methods be defined * <code>do_filters</code> - all filter operations for this node go here (e.g., <code>df.filter...</code>)</p> <ul> <li> <p><code>do_annotate</code> - all annotations go here (e.g., <code>df['zip'] = df.zipfull.apply(lambda x: x.split('-')[0])</code>)</p> </li> <li> <p><code>do_post_join_annotate</code> - annotations that require data from a child be joined in (e.g., need a delta between dates from two tables)</p> </li> <li> <p><code>do_normalize</code> - all anomaly filtering, data normalization, etc. go here (e.g., <code>df['val_norm'] = df['val'].apply(lambda x: x/df['val'].max())</code>)</p> </li> <li> <p><code>do_post_join_filters</code> - all filters requiring data from more than 1 table go here</p> </li> <li> <p><code>do_reduce</code> - all aggregation operations for features (e.g., <code>df.groupby(key).agg(...)</code>)</p> </li> <li> <p><code>do_labels</code> - any label-specific aggregation operations (e.g., <code>df.groupby(key).agg(had_order = 1)</code>)</p> </li> </ul>"},{"location":"abstractions/#dynamicnode","title":"DynamicNode","text":"<p>A dynamic node is any node that is instantiated without defined method  definitions.  This is useful for doing automated feature engineering.</p>"},{"location":"abstractions/#edge","title":"Edge","text":"<p>An edge is a relationship between two nodes.  This is typically a foreign key.  For  example if we had a <code>customers</code> table and <code>orders</code> table we would add an edge between the  <code>customers</code> node and the <code>orders</code> node:</p> <pre><code>gr.add_entity_edge(\n    parent_node=customer_node,\n    relation_node=orders_node,\n    parent_key='id',\n    relation_key='customer_id',\n    reduce=True\n</code></pre> <p>The <code>reduce</code> parameter tells <code>graphreduce</code> whether or not to execute  aggregation operations.</p>"},{"location":"abstractions/#graphreduce-graph","title":"GraphReduce Graph","text":"<p>The top-level <code>GraphReduce</code> class inherits directly from <code>networkx.DiGraph</code> to take advantage of many graph algorithms implemented in <code>networkx</code>.  The instances house to shared parameters for the entire graph of computation across all nodes and edges.</p> <p>Things such as the node to which to aggregate the data, the date for splitting the data, the compute layer (e.g., <code>pandas</code>, <code>dask</code>), the amount of history to include (365 days), the label period, whether or not to automate feature engineering, the label/target node and label/target column, etc.  All of these parameters get pushed down through the graph so we can do things like point in time correctness, etc.</p> <p>Since we inherit from <code>networkx</code> the API for adding nodes is unchanged: <pre><code>import datetime\nfrom graphreduce.graph_reduce import GraphReduce\nfrom grpahreduce.enum import PeriodUnit, ComputeLayerEnum\n\ngr = GraphReduce(\n    name='test',\n    parent_node=customer_node,\n    fmt='parquet',\n    compute_layer=ComputeLayerEnum.spark,\n    cut_date=datetime.datetime(2024, 7, 1),\n    compute_period_val=365,\n    compute_period_unit=PeriodUnit.day\n    auto_features=False,\n    label_node=order_node,\n    label_operation='sum',\n    label_field='order_total',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day,\n    spark_sqlctx=sqlCtx\n)\n\ngr.add_node(customer_node)\ngr.add_node(order_node)\ngr.add_node(notification_node)\ngr.add_node(...)\n...\n</code></pre></p>"},{"location":"tutorial_custom_graph/","title":"Defining node-level operations","text":"<p>Many times the automated primitives aren't enough and we want custom aggregation, filtering, normalization, and annotation.  In these cases we need to define operations somewhere.  Graphreduce takes the approach of centralizing operations in the node to which they pertain.</p> <p>For example, if we are defining a filter operation on the <code>orders.csv</code> feature definition that will live in a node defined for that dataset:</p> <pre><code>from graphreduce.node import GraphReduceNode\n\nclass OrderNode(GraphReduceNode):\n    def do_filters(self):\n        self.df = self.df[self.df[self.colabbr('amount')] &lt; 100000]    \n</code></pre> <p>By defining a node per dataset we can implement custom logic and focus only on the data of interest versus line 355 of a 2000 line SQL statement.</p>"},{"location":"tutorial_custom_graph/#full-node-implementation","title":"Full node implementation","text":"<p><code>graphreduce</code> prioritizes convention over configuration, so all <code>GraphReduceNode</code> subclasses must define the 7 required abstract methods, even if they do nothing.  One of the main reasons for enforcing this is so that as feature definitions evolve the location in which a particular operation needs to go should be clear.</p> <pre><code>class OrderNode(GraphReduceNode):\n    def do_filters(self):\n        self.df = self.df[self.df[self.colabbr('amount')] &lt; 100000]\n\n    def do_annotate(self):\n        pass\n\n    def do_normalize(self):\n        pass\n\n    def do_reduce(self, reduce_key):\n        return self.prep_for_features().groupby(self.colabbr(reduce_key)).agg(\n            **{\n                self.colabbr('max_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='max'),\n                self.colabbr('min_amount'): pd.NamedAgg(column=self.colabbr('amount'), aggfunc='min')\n            }\n        )\n\n    def do_labels(self, reduce_key):\n        pass\n\n    def do_post_join_annotate(self):\n        pass\n\n    def do_post_join_filters(self):\n        pass\n</code></pre>"},{"location":"tutorial_custom_graph/#reduce-operations","title":"Reduce operations","text":"<p>If we want any aggregation to happen on this node we need to define <code>do_reduce</code>.  In this case we are computing the <code>min</code> and <code>max</code> of the column called <code>amount</code>. There are two helper methods used in the above code snippet that deserve elaboration:</p> <ul> <li><code>self.colabbr</code> which is <code>GraphReduceNode.colabbr</code> - this method just uses the prefix parameterized for this node so a column like <code>'amount'</code> will now be <code>'ord_amount'</code> if the prefix is <code>'ord'</code></li> <li><code>self.prep_for_features</code> which is <code>GraphReduceNode.prep_for_features</code> - this method filters the dataframe by the <code>cut_date</code> and <code>compute_period_val</code> if the data is time series.  If the data is not time series it just returns the full dataframe.</li> </ul>"},{"location":"tutorial_custom_graph/#label-target-generation","title":"Label / target generation","text":"<p>If this node happens to be the node from which the label originates, we need to implement the </p>"},{"location":"tutorial_index/","title":"Quickstart","text":"<p>The most simple graph is two nodes with no time component, no aggregations, and no labels.  We are using customer (<code>cust.csv</code>) and order data (<code>orders.csv</code>). To see the full code go here link</p> <p>In plain terms what the below code does is as follows:</p> <ol> <li>Create prefixes for each node so we always know where the column originated from after joinining the two datasets.</li> <li>Instantiate two <code>DynamicNode</code> instances: one node for <code>cust.csv</code> and another for <code>orders.csv</code>.</li> <li>Instantiate the <code>GraphReduce</code> object to house the compute graph.  We are specifying that the <code>cust.csv</code> node is the <code>parent_node</code>, which means all data will be joined to and aggregated to the <code>cust.csv</code> node.  In cases where we reduce the data the resulting dataset should be at the ganularity of the <code>parent_node</code> dimension.</li> <li>Add the nodes.</li> <li>Add the edge between the nodes.</li> <li>Execute the computation with <code>GraphReduce.do_transformations()</code> the primary entrypoint to execution.</li> <li>Dump out the head of the computed dataframe. </li> </ol> <pre><code>from graphreduce.node import GraphReduceNode, DynamicNode\nfrom graphreduce.graph_reduce import GraphReduce\nfrom graphreduce.enum import ComputeLayerEnum as GraphReduceComputeLayerEnum, PeriodUnit\n\n# Need unique prefixes for all nodes\n# so when columns are merged we know\n# where they originate from.\nprefixes = {\n    'cust.csv' : {'prefix':'cu'},\n    'orders.csv':{'prefix':'ord'}\n}\n\n# create graph reduce nodes\ngr_nodes = {\n    f.split('/')[-1]: DynamicNode(\n        fpath=f,\n        fmt='csv',\n        pk='id',\n        prefix=prefixes[f]['prefix'],\n        date_key=None,\n        compute_layer=GraphReduceComputeLayerEnum.pandas,\n        compute_period_val=730,\n        compute_period_unit=PeriodUnit.day,\n    )\n    for f in files.keys()\n}\n\n# Instantiate GraphReduce with params.\n# We are using 'cust.csv' as parent node\n# so the granularity should be at the customer\n# dimension.\ngr = GraphReduce(\n    name='starter_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    auto_features=True,\n    auto_feature_hops_front=1,\n    auto_feature_hops_back=2,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=60,\n    label_period_unit=PeriodUnit.day\n)\n\n# Add the nodes to the GraphReduce instance.\ngr.add_node(gr_nodes['cust.csv'])\ngr.add_node(gr_nodes['orders.csv'])\n\ngr.add_entity_edge(\n    parent_node=gr_nodes['cust.csv'],\n    relation_node=gr_nodes['orders.csv'],\n    parent_key='id',\n    relation_key='customer_id',\n    reduce=True\n)\n\ngr.do_transformations()\n\ngr.parent_node.df.head()\n    cu_id   cu_name ord_customer_id ord_id_count    ord_customer_id_count   ord_ts_min  ord_ts_max  ord_amount_count    ord_customer_id_dupe    ord_id_label\n0   1   wes 1   3   3   2023-05-12  2023-09-02  3   1   3\n1   2   ana 2   3   3   2022-08-05  2023-10-15  3   2   3\n2   3   caleb   3   1   1   2023-06-01  2023-06-01  1   3   1\n3   4   luly    4   2   2   2024-01-01  2024-02-01  2   4   2\n</code></pre>"},{"location":"tutorial_pandas_dask/","title":"Pandas and Dask","text":""},{"location":"tutorial_time/","title":"Point in time correctness","text":"<p>To handle point in time correctness properly all nodes  in the graph need to share the same date parameters.</p> <p>Using the example from before with <code>cust.csv</code> and <code>orders.csv</code> let's say we want to only compute features within 6 months and compute a label for 45 days.  </p> <p>In the <code>GraphReduce</code> instance we specify <code>compute_period_val</code> and <code>label_period_val</code>. These parameters control how much history is included during execution.  For this graph data from <code>2023/9/1</code> going back 180 days will be included in feature preparation and data from <code>2023/9/1</code> going forward 45 days will be included in label preparation.</p> <pre><code>gr = GraphReduce(\n    name='starter_graph',\n    parent_node=gr_nodes['cust.csv'],\n    fmt='csv',\n    cut_date=datetime.datetime(2023,9,1),\n    compute_layer=GraphReduceComputeLayerEnum.pandas,\n    compute_period_val=180,\n    compute_period_unit=PeriodUnit.day,\n    auto_features=True,\n    label_node=gr_nodes['orders.csv'],\n    label_operation='count',\n    label_field='id',\n    label_period_val=45,\n    label_period_unit=PeriodUnit.day\n)\n</code></pre> <p>As you may gather, this allows us to simply change a couple of parameters to regenerate datasets with different time periods.  </p>"}]}